# 导入必要的库
library(readr)
library(tidyr)
library(ggplot2)
library(dplyr)

# 导入数据集
setwd("C:/Users/adilai/Desktop/数据科学大作业")
postings <- read.csv("postings.csv",header = T)
is.data.frame(postings)

#查看缺失值
sapply(postings, function (x) sum(is.na (x)))

#分析缺失值:
  #salary等列缺失值较大
  #remote_allowed有两类，即1和NA，因此在最后数据整合后将NA改为0。
distinct(postings, remote_allowed)
  #与之相同处理views，applies
  #时间列分析。发现时间几乎是从12月到4月。

#（1）首先处理薪资问题
head(postings[,c("max_salary","med_salary","min_salary")],5)
#利用最高薪资和最低薪资来med_salary填充数据
postings$med_salary <- ifelse(is.na(postings$med_salary), (postings$max_salary
+ postings$min_salary) / 2, postings$med_salary)

#重新查看缺失值
sapply(postings,function(x) sum(is.na(x)))
#根据结果可以看到现在med_salary缺失值已经从117569降到87776
#相比其他数据好很多

#导入salaries数据集
#salaries <- read.csv("jobs/salaries.csv")

#利用最高薪资和最低薪资来填充数据
#salaries$med_salary <- ifelse(is.na(salaries$med_salary), (salaries$max_salary
#+ salaries$min_salary) / 2, salaries$med_salary)

#查看缺失
#sapply(salaries,function(x) sum(is.na(x)))

#salaries <- subset(salaries,select =-c(salary_id,max_salary,min_salary,currency,
#                                     compensation_type))

# 使用 left_join 函数合并数据集，并指定你想要合并的列  
#merged_data <- left_join(postings, salaries, by = "job_id") 

#salary  <- merged_data[c("med_salary.x", "med_salary.y")]

#查看缺失值
#sapply(salary,function (x) sum(is.na (x)))
#结果相同因此删除merge_data
#rm(merged_data)
#rm(salary)
#rm(salaries)
#因此不需要合并

#使用相同的方法处理benefits
#导入benefits
benefits <- read.csv("jobs/benefits.csv")

#删除不需要的列—inferred
benefits <- subset(benefits,select = -c(inferred))

# 使用dplyr包进行分组和聚合  
benefits <- benefits %>%  
  group_by(job_id) %>%  
  summarise(type = toString(unique(type)), .groups = 'drop')  
#合并benefits和postings
postings_benefits <- left_join(postings, benefits, by = "job_id")


#开始处理skill
#根据linkedle页面分析，skill很重要
#导入job_skill
job_skills <- read.csv("jobs/job_skills.csv")
skills <- read.csv("mappings/skills.csv")

# 执行左连接  
job_skills <- merge(job_skills, skills, by = "skill_abr", all.x = TRUE) 

#删除不需要的列skill_abr
job_skills <- subset(job_skills,select=-c(skill_abr))

# 查看发现一个job匹配多个skills，因此使用dplyr包进行分组和聚合  
job_skills <- job_skills %>%  
  group_by(job_id) %>%  
  summarise( skill_name = toString(unique(skill_name)), .groups = 'drop')  

# 显示聚合后的结果的前6行  
head(job_skills)

# 使用 left_join 函数合并数据集，并指定合并postings和left_join  
postings_benefits_skills <- left_join(postings_benefits, job_skills, by = "job_id") 



#职位行业数据整理——可以分析那个行业发布的数据最多
#导入数据
job_industries <- read.csv("jobs/job_industries.csv")
industries <- read.csv("mappings/industries.csv")
# 执行左连接  
industry <- merge(job_industries, industries, by = "industry_id", all.x = TRUE) 
#删除多于的列industry_id
industry <- subset(industry,select=-c(industry_id))
#查看一下job_id是否是唯一
# 查看job_id是否都是唯一的  
unique_job_id1 <- !any(duplicated(industry$job_id))
unique_job_id1
#结果是false，则存在重复的job_id
#看看是否有重复行
has_duplicates <- anyDuplicated(industry)  

if (has_duplicates > 0) {  
  print("存在重复行")  
} else {  
  print("没有重复行")  
}
#结果是没有重复行，那么就说明一个job_id存在对应多个行业#。
# 使用dplyr包进行分组和聚合 
industry<-  industry%>%  
  group_by(job_id) %>%  
  summarise(industries_name = toString(unique(industry_name)), .groups = 'drop')
#查看industry
head(industry)
#合并postings_skills_benefits)和industry
postings_skills_benefits_industry <- left_join( postings_benefits_skills,industry
                                                ,by = "job_id")



#公司数据整理！！！！
#数据导入
data <- postings_skills_benefits_industry
companies <- read.csv("companies/companies.csv",header = T)
#删除companies中不需要的列：
companies <- subset(companies,select = -c(state,zip_code,address,city))
#company_specialities <- read.csv("companies/company_specialities.csv")
company_industries <- read.csv("companies/company_industries.csv")
employee_counts <- read.csv("companies/employee_counts.csv")
#分组聚合=====觉得这个company_specialities没有什么研究价值，有company_industries。
#company_specialities<-  company_specialities%>%  
 # group_by(company_id) %>%  
  #summarise(type = toString(unique(speciality)), .groups = 'drop')
#因此删除company_specialities数据集
#rm(company_specialities)
data <- left_join(data,companies,by="company_id")
#data <- left_join(data,company_specialities,by="company_id")
#data <- left_join(data,company_industries,by="company_id")
#这里会显示多对多问题，所以company_industries中可能存在一个公司对应不同行业，所以进行聚合处理
company_industries<-  company_industries%>%  
  group_by(company_id) %>%  
  summarise(type = toString(unique(industry)), .groups = 'drop')
#再次运行发现问题解决了！
data <- left_join(data,company_industries,by="company_id")
#连接data和employee_counts数据集时删除必要的列，再连接，删除time_recorded
#employee_counts <- subset(employee_counts,select=-c(time_recorded))
#连接data和employee_counts数据集
#data <- left_join(data,employee_counts,by="company_id")
#结果发现有多对多的关系，看数据发现，一个公司的数据是有不同的时间抽取的，所以这里取每一个的最新抽取时间
#首先对time_recorded进行处理，转换成时间数据
# 加载 dplyr 包（如果需要的话）  
# install.packages("dplyr")  
library(dplyr)  

# 假设 employee_counts$time_recorded 已经是秒级时间戳  
# 将其转换为 POSIXct 日期时间对象  
employee_counts$time_recorded <- as.POSIXct(employee_counts$time_recorded, origin = "1970-01-01", tz = "UTC")  

# 现在，你可以按 company_id 和最新的 time_recorded 选择数据  
latest_counts <- employee_counts %>%  
  group_by(company_id) %>%  
  slice_max(order_by = time_recorded, n = 1) %>%  
  ungroup()  

# 查看结果
#这样就抽取成功了
#再进行连接
#发现还是出现多对多，发现有重复值，删除
 
latest_counts <- latest_counts %>%  
  distinct(company_id, .keep_all = TRUE)  

# 查看结果  
print(latest_counts)
data <- left_join(data,latest_counts,by="company_id")
#终于不再报多对多了！
#至此所有数据合并完毕-----可喜可贺


#查看所有列的缺失值
sapply(data, function (x) sum(is.na (x)))


#至此工作数据整合完毕
#现在需要做的就是删除不需要的列，整理
#1.首先删除不需要的列
#采用dplyr中的select函数
data<- subset(data,select = -c(job_posting_url,application_url,application_type,
                               expiry,closed_time,formatted_experience_level,
                               skills_desc,listed_time, posting_domain, currency,
                               compensation_type,time_recorded,url,name))

#说明混淆变量
#industries_name是职位的。type.x是benefit的，type.y是公司的。description.x是职位的，description.y是公司的
#后面会改名
#根据缺失值进行处理
sapply(data, function (x) sum(is.na (x)))
#删除有缺失值的skill_name,industries_name,description.y,country,type.y,company_id
#employee_count,follower_count
#删除company_id为空的行
data <- data[!is.na(data$company_id), ] 
data <- data[!is.na(data$skill_name), ] 
data <- data[!is.na(data$industries_name), ] 
data <- data[!is.na(data$description.y), ] 
data <- data[!is.na(data$country), ] 
data <- data[!is.na(data$type.y), ] 
data <- data[!is.na(data$employee_count), ] 
data <- data[!is.na(data$follower_count), ] 

#数据已经从123849变为120442，删了将近3000数据
#现在查看一下缺失值
sapply(data, function (x) sum(is.na (x)))
#问题在于人数与规模不相符，公司规模有缺失值-4567。因此删除这一列~
data <- subset(data,select=-c(company_size))
#查看是否有空值
# 使用sapply函数计算每列空字符串的数量  
sapply(data, function(x) sum(x == "")) 
#pay_period缺失值太大，不管。
#根据结果显示，删除有空值的行~description.x company_name,description.y
# 删除company_name为空字符串的行  
data <- data %>%  
  filter(description.x != "" &  company_name != "" & description.y != "")  

#数据从120442变为119469
# 改变存储日期时间对象  
data$original_listed_time <- as.POSIXct(data$original_listed_time/ 1000, origin = "1970-01-01", tz = "UTC")  


#选取了3月20号到4月20号的一个月数据！
# 导入需要的库
library(lubridate)
tart_date <- ymd_hms("2024-3-20 00:00:00")  # 开始日期

# 使用ymd_hms函数解析日期和时间  
end_date <- ymd_hms("2024-04-20 00:26:43")

data <- data[data$original_listed_time>= tart_date & data$original_listed_time <= end_date, ]

#查看缺失值处理其他问题
sapply(data, function (x) sum(is.na (x)))
#缺失值主要集中在各种salary,views, applies,remote_allowed,type.x
#将remote_allowed中的缺失值改为0
data$remote_allowed[is.na(data$remote_allowed)] <- 0

#将applies中的缺失值改为0
data$applies[is.na(data$applies)] <- 0

#将views中的缺失值改为0
data$views[is.na(data$views)] <- 0

#将type中的缺失值改为0
data$type.x[is.na(data$type.x)] <- 0

#再查看所有列的缺失值
sapply(data, function (x) sum(is.na (x)))

#工资不做处理
#改变量名称
colnames(data)[colnames(data) == "description.x"] <- "P_description"   #指向职位描述
colnames(data)[colnames(data) == "description.y"] <- "C_description" #指向公司描述
colnames(data)[colnames(data) == "original_listed_time"] <- "isted_time" #职位发出时间
colnames(data)[colnames(data) == "skill_name"] <- "skills" #技能
colnames(data)[colnames(data) == "industries_name"] <- "P_industry" #指向职位的行业领域
colnames(data)[colnames(data) == "type.x"] <- "benefit_tyoe" #福利
colnames(data)[colnames(data) == "type.y"] <- "C_industry" 
# 查看job_id是否都是唯一的  
unique_job_id <- !any(duplicated(data$job_id))
unique_job_id
#结果是true，说明job_id唯一的。
#终于除了salary和pay_period都解决完啦
#前面的数据已经好了就用这个data


#嘿嘿又出现问题啦，删除各种salary和pay_period
#因为大家用的都是merged_data,因此在这里换一下
merged_data <- subset(data,select=-c(min_salary,max_salary,med_salary,pay_period))

#用这个merged_data
#完成啦







#--------------------------------------------------------------------------

#这里是公司数据提取
#这里可以看看，也可以直接用merged_data,不用company





#抽取公司数据的事情！
# 先提取公司的数据，company_id, C_description,C_industry,country,employee_count,follower_count
distinct_companies <- data %>%  
  group_by(job_id) %>%  
  distinct(company_id, C_description,C_industry,country,employee_count,follower_count) %>%  # 列出你想要保留的所有列名  
  ungroup() %>%  
  select(-job_id)
#查看是否有重复值
unique_distinct_company<- !any(duplicated(distinct_companies$company_id))
unique_distinct_company
#结果是false则有重复值
#则进行删除
#查看是否是数据框
is.data.frame(distinct_companies)
# 删除所有按company_id重复的行  
company<- distinct_companies %>%  
  distinct(company_id, .keep_all = TRUE)  
#再查看company_id是否是唯一的
unique_company<- !any(duplicated(company$company_id))
unique_company
#结果是true,证明提取成功啦
# 如果job_id都是唯一的，那么你可能不需要这个额外的步骤，  
# 因为每个job_id只会对应一个company_id（假设没有NA或重复）
